{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we explore how dimensionality of the embeddings affects the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import glob\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "from PIL import Image\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "# We use CUDA if possible\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"CUDA is available:\", cuda)\n",
    "device = torch.device('cuda' if cuda else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we create the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation made following https://github.com/adambielski/siamese-triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set is: MIT_SPLIT\n",
      "{'Opencountry': 0, 'coast': 1, 'forest': 2, 'highway': 3, 'inside_city': 4, 'mountain': 5, 'street': 6, 'tallbuilding': 7}\n",
      "torch.Size([1881, 3, 256, 256])\n",
      "torch.Size([1881])\n"
     ]
    }
   ],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "    \n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "        \n",
    "        if len(img1.shape) > 2:\n",
    "            img1 = Image.fromarray(np.uint8(img1.permute(1, 2, 0).numpy() * 255))\n",
    "            img2 = Image.fromarray(np.uint8(img2.permute(1, 2, 0).numpy() * 255))\n",
    "        else:\n",
    "            # In the case of the MNIST dataset image was already in range [0-255]\n",
    "            img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "            img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        return (img1, img2), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "    \n",
    "### HYPERPARAMETERS\n",
    "train_set = 'MIT_SPLIT'\n",
    "\n",
    "print('Training set is:', train_set)\n",
    "root_dir = '../datasets/' + train_set\n",
    "\n",
    "train_data_dir= root_dir + '/train'\n",
    "val_data_dir= root_dir + '/test'\n",
    "test_data_dir= root_dir + '/test'\n",
    "\n",
    "img_width = 224\n",
    "img_height=224\n",
    "\n",
    "\n",
    "### CREATE DATASET\n",
    "transformation_train = transform.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    # transform.RandomRotation(90),\n",
    "    # transform.RandomHorizontalFlip(),\n",
    "    # transform.RandomVerticalFlip(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# No need to use data augmentation in validation\n",
    "transformation_val = transform.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_data_dir, transform=transformation_train)\n",
    "valid_dataset = torchvision.datasets.ImageFolder(root=test_data_dir,  transform=transformation_val)\n",
    "\n",
    "original_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "original_test_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# We need to fill some attributes for the SiameseDataset class\n",
    "train_dataset.train = True\n",
    "valid_dataset.train = False\n",
    "\n",
    "train_dataset.train_labels = torch.from_numpy(np.array(train_dataset.targets))\n",
    "valid_dataset.test_labels = torch.from_numpy(np.array(valid_dataset.targets))\n",
    "\n",
    "train_dataset.train_data = torch.from_numpy(np.array([s[0].numpy() for s in train_dataset]))\n",
    "valid_dataset.test_data = torch.from_numpy(np.array([s[0].numpy() for s in valid_dataset]))\n",
    "\n",
    "\n",
    "inv_class_to_idx = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "\n",
    "print(train_dataset.class_to_idx)\n",
    "print(train_dataset.train_data.shape)\n",
    "print(train_dataset.train_labels.shape)\n",
    "\n",
    "siamese_train_dataset = SiameseDataset(train_dataset) # Returns pairs of images and target same/different\n",
    "siamese_test_dataset = SiameseDataset(valid_dataset)\n",
    "\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classes = train_dataset.classes\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, title='', save_name='', xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title)\n",
    "    for i in range(8):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(mnist_classes, loc=\"upper right\")\n",
    "    if save_name != '':\n",
    "        plt.savefig(f'./outputs_task_b/{save_name}')\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create our Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = 0.5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/1881 (0%)]\tLoss: 0.246699\n",
      "Train: [800/1881 (42%)]\tLoss: 0.147226\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.124329\n",
      "Epoch: 1/30. Train set: Average loss: 0.1347\n",
      "Epoch: 1/30. Validation set: Average loss: 0.1296\n",
      "Train: [0/1881 (0%)]\tLoss: 0.059445\n",
      "Train: [800/1881 (42%)]\tLoss: 0.120591\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.123343\n",
      "Epoch: 2/30. Train set: Average loss: 0.1216\n",
      "Epoch: 2/30. Validation set: Average loss: 0.1271\n",
      "Train: [0/1881 (0%)]\tLoss: 0.094988\n",
      "Train: [800/1881 (42%)]\tLoss: 0.116766\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.116564\n",
      "Epoch: 3/30. Train set: Average loss: 0.1149\n",
      "Epoch: 3/30. Validation set: Average loss: 0.1151\n",
      "Train: [0/1881 (0%)]\tLoss: 0.072679\n",
      "Train: [800/1881 (42%)]\tLoss: 0.115529\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.108124\n",
      "Epoch: 4/30. Train set: Average loss: 0.1115\n",
      "Epoch: 4/30. Validation set: Average loss: 0.1145\n",
      "Train: [0/1881 (0%)]\tLoss: 0.061500\n",
      "Train: [800/1881 (42%)]\tLoss: 0.103319\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.100663\n",
      "Epoch: 5/30. Train set: Average loss: 0.1017\n",
      "Epoch: 5/30. Validation set: Average loss: 0.1074\n",
      "Train: [0/1881 (0%)]\tLoss: 0.077820\n",
      "Train: [800/1881 (42%)]\tLoss: 0.095507\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.097932\n",
      "Epoch: 6/30. Train set: Average loss: 0.0963\n",
      "Epoch: 6/30. Validation set: Average loss: 0.1050\n",
      "Train: [0/1881 (0%)]\tLoss: 0.081027\n",
      "Train: [800/1881 (42%)]\tLoss: 0.089254\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.088534\n",
      "Epoch: 7/30. Train set: Average loss: 0.0885\n",
      "Epoch: 7/30. Validation set: Average loss: 0.0990\n",
      "Train: [0/1881 (0%)]\tLoss: 0.026350\n",
      "Train: [800/1881 (42%)]\tLoss: 0.079084\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.071490\n",
      "Epoch: 8/30. Train set: Average loss: 0.0758\n",
      "Epoch: 8/30. Validation set: Average loss: 0.0978\n",
      "Train: [0/1881 (0%)]\tLoss: 0.096807\n",
      "Train: [800/1881 (42%)]\tLoss: 0.072927\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.067478\n",
      "Epoch: 9/30. Train set: Average loss: 0.0695\n",
      "Epoch: 9/30. Validation set: Average loss: 0.0975\n",
      "Train: [0/1881 (0%)]\tLoss: 0.065641\n",
      "Train: [800/1881 (42%)]\tLoss: 0.069947\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.073994\n",
      "Epoch: 10/30. Train set: Average loss: 0.0722\n",
      "Epoch: 10/30. Validation set: Average loss: 0.0951\n",
      "Train: [0/1881 (0%)]\tLoss: 0.052939\n",
      "Train: [800/1881 (42%)]\tLoss: 0.063580\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.067363\n",
      "Epoch: 11/30. Train set: Average loss: 0.0659\n",
      "Epoch: 11/30. Validation set: Average loss: 0.0959\n",
      "Train: [0/1881 (0%)]\tLoss: 0.046875\n",
      "Train: [800/1881 (42%)]\tLoss: 0.063261\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.066953\n",
      "Epoch: 12/30. Train set: Average loss: 0.0643\n",
      "Epoch: 12/30. Validation set: Average loss: 0.0938\n",
      "Train: [0/1881 (0%)]\tLoss: 0.062743\n",
      "Train: [800/1881 (42%)]\tLoss: 0.061435\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.062981\n",
      "Epoch: 13/30. Train set: Average loss: 0.0625\n",
      "Epoch: 13/30. Validation set: Average loss: 0.0944\n",
      "Train: [0/1881 (0%)]\tLoss: 0.062514\n",
      "Train: [800/1881 (42%)]\tLoss: 0.058862\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.064047\n",
      "Epoch: 14/30. Train set: Average loss: 0.0615\n",
      "Epoch: 14/30. Validation set: Average loss: 0.0938\n",
      "Train: [0/1881 (0%)]\tLoss: 0.047807\n",
      "Train: [800/1881 (42%)]\tLoss: 0.058961\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.058283\n",
      "Epoch: 15/30. Train set: Average loss: 0.0577\n",
      "Epoch: 15/30. Validation set: Average loss: 0.0957\n",
      "Train: [0/1881 (0%)]\tLoss: 0.022024\n",
      "Train: [800/1881 (42%)]\tLoss: 0.056385\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.056007\n",
      "Epoch: 16/30. Train set: Average loss: 0.0556\n",
      "Epoch: 16/30. Validation set: Average loss: 0.0934\n",
      "Train: [0/1881 (0%)]\tLoss: 0.082923\n",
      "Train: [800/1881 (42%)]\tLoss: 0.050918\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.052781\n",
      "Epoch: 17/30. Train set: Average loss: 0.0532\n",
      "Epoch: 17/30. Validation set: Average loss: 0.0941\n",
      "Train: [0/1881 (0%)]\tLoss: 0.005729\n",
      "Train: [800/1881 (42%)]\tLoss: 0.060363\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.050500\n",
      "Epoch: 18/30. Train set: Average loss: 0.0553\n",
      "Epoch: 18/30. Validation set: Average loss: 0.0935\n",
      "Train: [0/1881 (0%)]\tLoss: 0.080871\n",
      "Train: [800/1881 (42%)]\tLoss: 0.051456\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049319\n",
      "Epoch: 19/30. Train set: Average loss: 0.0516\n",
      "Epoch: 19/30. Validation set: Average loss: 0.0931\n",
      "Train: [0/1881 (0%)]\tLoss: 0.054618\n",
      "Train: [800/1881 (42%)]\tLoss: 0.053793\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.058442\n",
      "Epoch: 20/30. Train set: Average loss: 0.0564\n",
      "Epoch: 20/30. Validation set: Average loss: 0.0944\n",
      "Train: [0/1881 (0%)]\tLoss: 0.019791\n",
      "Train: [800/1881 (42%)]\tLoss: 0.053507\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.054072\n",
      "Epoch: 21/30. Train set: Average loss: 0.0531\n",
      "Epoch: 21/30. Validation set: Average loss: 0.0934\n",
      "Train: [0/1881 (0%)]\tLoss: 0.078330\n",
      "Train: [800/1881 (42%)]\tLoss: 0.058703\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.052564\n",
      "Epoch: 22/30. Train set: Average loss: 0.0552\n",
      "Epoch: 22/30. Validation set: Average loss: 0.0930\n",
      "Train: [0/1881 (0%)]\tLoss: 0.071187\n",
      "Train: [800/1881 (42%)]\tLoss: 0.054410\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049330\n",
      "Epoch: 23/30. Train set: Average loss: 0.0521\n",
      "Epoch: 23/30. Validation set: Average loss: 0.0928\n",
      "Train: [0/1881 (0%)]\tLoss: 0.052877\n",
      "Train: [800/1881 (42%)]\tLoss: 0.051561\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.052731\n",
      "Epoch: 24/30. Train set: Average loss: 0.0515\n",
      "Epoch: 24/30. Validation set: Average loss: 0.0929\n",
      "Train: [0/1881 (0%)]\tLoss: 0.047555\n",
      "Train: [800/1881 (42%)]\tLoss: 0.054448\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.052126\n",
      "Epoch: 25/30. Train set: Average loss: 0.0522\n",
      "Epoch: 25/30. Validation set: Average loss: 0.0930\n",
      "Train: [0/1881 (0%)]\tLoss: 0.089192\n",
      "Train: [800/1881 (42%)]\tLoss: 0.052863\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.051715\n",
      "Epoch: 26/30. Train set: Average loss: 0.0517\n",
      "Epoch: 26/30. Validation set: Average loss: 0.0930\n",
      "Train: [0/1881 (0%)]\tLoss: 0.037052\n",
      "Train: [800/1881 (42%)]\tLoss: 0.053843\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.050981\n",
      "Epoch: 27/30. Train set: Average loss: 0.0522\n",
      "Epoch: 27/30. Validation set: Average loss: 0.0930\n",
      "Train: [0/1881 (0%)]\tLoss: 0.043135\n",
      "Train: [800/1881 (42%)]\tLoss: 0.055324\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049658\n",
      "Epoch: 28/30. Train set: Average loss: 0.0520\n",
      "Epoch: 28/30. Validation set: Average loss: 0.0931\n",
      "Train: [0/1881 (0%)]\tLoss: 0.132335\n",
      "Train: [800/1881 (42%)]\tLoss: 0.052685\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.050072\n",
      "Epoch: 29/30. Train set: Average loss: 0.0524\n",
      "Epoch: 29/30. Validation set: Average loss: 0.0932\n",
      "Train: [0/1881 (0%)]\tLoss: 0.032219\n",
      "Train: [800/1881 (42%)]\tLoss: 0.050586\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.051685\n",
      "Epoch: 30/30. Train set: Average loss: 0.0514\n",
      "Epoch: 30/30. Validation set: Average loss: 0.0932\n",
      "Train: [0/1881 (0%)]\tLoss: 0.321031\n",
      "Train: [800/1881 (42%)]\tLoss: 0.127286\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.118941\n",
      "Epoch: 1/30. Train set: Average loss: 0.1234\n",
      "Epoch: 1/30. Validation set: Average loss: 0.1081\n",
      "Train: [0/1881 (0%)]\tLoss: 0.141698\n",
      "Train: [800/1881 (42%)]\tLoss: 0.106983\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.107634\n",
      "Epoch: 2/30. Train set: Average loss: 0.1058\n",
      "Epoch: 2/30. Validation set: Average loss: 0.1135\n",
      "Train: [0/1881 (0%)]\tLoss: 0.093776\n",
      "Train: [800/1881 (42%)]\tLoss: 0.098496\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.096516\n",
      "Epoch: 3/30. Train set: Average loss: 0.0972\n",
      "Epoch: 3/30. Validation set: Average loss: 0.1126\n",
      "Train: [0/1881 (0%)]\tLoss: 0.100114\n",
      "Train: [800/1881 (42%)]\tLoss: 0.089716\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.087882\n",
      "Epoch: 4/30. Train set: Average loss: 0.0864\n",
      "Epoch: 4/30. Validation set: Average loss: 0.0977\n",
      "Train: [0/1881 (0%)]\tLoss: 0.077887\n",
      "Train: [800/1881 (42%)]\tLoss: 0.076142\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.074476\n",
      "Epoch: 5/30. Train set: Average loss: 0.0765\n",
      "Epoch: 5/30. Validation set: Average loss: 0.0983\n",
      "Train: [0/1881 (0%)]\tLoss: 0.055657\n",
      "Train: [800/1881 (42%)]\tLoss: 0.075057\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.068707\n",
      "Epoch: 6/30. Train set: Average loss: 0.0709\n",
      "Epoch: 6/30. Validation set: Average loss: 0.0943\n",
      "Train: [0/1881 (0%)]\tLoss: 0.072875\n",
      "Train: [800/1881 (42%)]\tLoss: 0.056355\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.062707\n",
      "Epoch: 7/30. Train set: Average loss: 0.0583\n",
      "Epoch: 7/30. Validation set: Average loss: 0.0937\n",
      "Train: [0/1881 (0%)]\tLoss: 0.025748\n",
      "Train: [800/1881 (42%)]\tLoss: 0.052092\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049361\n",
      "Epoch: 8/30. Train set: Average loss: 0.0502\n",
      "Epoch: 8/30. Validation set: Average loss: 0.0817\n",
      "Train: [0/1881 (0%)]\tLoss: 0.018792\n",
      "Train: [800/1881 (42%)]\tLoss: 0.045614\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.043000\n",
      "Epoch: 9/30. Train set: Average loss: 0.0434\n",
      "Epoch: 9/30. Validation set: Average loss: 0.0807\n",
      "Train: [0/1881 (0%)]\tLoss: 0.048167\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040968\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041642\n",
      "Epoch: 10/30. Train set: Average loss: 0.0419\n",
      "Epoch: 10/30. Validation set: Average loss: 0.0812\n",
      "Train: [0/1881 (0%)]\tLoss: 0.024461\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041087\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037553\n",
      "Epoch: 11/30. Train set: Average loss: 0.0393\n",
      "Epoch: 11/30. Validation set: Average loss: 0.0796\n",
      "Train: [0/1881 (0%)]\tLoss: 0.041268\n",
      "Train: [800/1881 (42%)]\tLoss: 0.038752\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039031\n",
      "Epoch: 12/30. Train set: Average loss: 0.0386\n",
      "Epoch: 12/30. Validation set: Average loss: 0.0816\n",
      "Train: [0/1881 (0%)]\tLoss: 0.042839\n",
      "Train: [800/1881 (42%)]\tLoss: 0.036837\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037926\n",
      "Epoch: 13/30. Train set: Average loss: 0.0361\n",
      "Epoch: 13/30. Validation set: Average loss: 0.0815\n",
      "Train: [0/1881 (0%)]\tLoss: 0.061012\n",
      "Train: [800/1881 (42%)]\tLoss: 0.032426\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.036436\n",
      "Epoch: 14/30. Train set: Average loss: 0.0345\n",
      "Epoch: 14/30. Validation set: Average loss: 0.0793\n",
      "Train: [0/1881 (0%)]\tLoss: 0.089813\n",
      "Train: [800/1881 (42%)]\tLoss: 0.033468\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.032533\n",
      "Epoch: 15/30. Train set: Average loss: 0.0328\n",
      "Epoch: 15/30. Validation set: Average loss: 0.0838\n",
      "Train: [0/1881 (0%)]\tLoss: 0.045924\n",
      "Train: [800/1881 (42%)]\tLoss: 0.031509\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.030643\n",
      "Epoch: 16/30. Train set: Average loss: 0.0318\n",
      "Epoch: 16/30. Validation set: Average loss: 0.0816\n",
      "Train: [0/1881 (0%)]\tLoss: 0.037967\n",
      "Train: [800/1881 (42%)]\tLoss: 0.029247\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028664\n",
      "Epoch: 17/30. Train set: Average loss: 0.0282\n",
      "Epoch: 17/30. Validation set: Average loss: 0.0815\n",
      "Train: [0/1881 (0%)]\tLoss: 0.014108\n",
      "Train: [800/1881 (42%)]\tLoss: 0.030305\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.030082\n",
      "Epoch: 18/30. Train set: Average loss: 0.0299\n",
      "Epoch: 18/30. Validation set: Average loss: 0.0805\n",
      "Train: [0/1881 (0%)]\tLoss: 0.041155\n",
      "Train: [800/1881 (42%)]\tLoss: 0.029517\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.029108\n",
      "Epoch: 19/30. Train set: Average loss: 0.0298\n",
      "Epoch: 19/30. Validation set: Average loss: 0.0808\n",
      "Train: [0/1881 (0%)]\tLoss: 0.035106\n",
      "Train: [800/1881 (42%)]\tLoss: 0.028220\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028703\n",
      "Epoch: 20/30. Train set: Average loss: 0.0290\n",
      "Epoch: 20/30. Validation set: Average loss: 0.0805\n",
      "Train: [0/1881 (0%)]\tLoss: 0.030951\n",
      "Train: [800/1881 (42%)]\tLoss: 0.026719\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028367\n",
      "Epoch: 21/30. Train set: Average loss: 0.0273\n",
      "Epoch: 21/30. Validation set: Average loss: 0.0805\n",
      "Train: [0/1881 (0%)]\tLoss: 0.027557\n",
      "Train: [800/1881 (42%)]\tLoss: 0.027092\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028553\n",
      "Epoch: 22/30. Train set: Average loss: 0.0281\n",
      "Epoch: 22/30. Validation set: Average loss: 0.0804\n",
      "Train: [0/1881 (0%)]\tLoss: 0.017353\n",
      "Train: [800/1881 (42%)]\tLoss: 0.024363\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028075\n",
      "Epoch: 23/30. Train set: Average loss: 0.0271\n",
      "Epoch: 23/30. Validation set: Average loss: 0.0806\n",
      "Train: [0/1881 (0%)]\tLoss: 0.018107\n",
      "Train: [800/1881 (42%)]\tLoss: 0.027653\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.027977\n",
      "Epoch: 24/30. Train set: Average loss: 0.0277\n",
      "Epoch: 24/30. Validation set: Average loss: 0.0806\n",
      "Train: [0/1881 (0%)]\tLoss: 0.020899\n",
      "Train: [800/1881 (42%)]\tLoss: 0.026993\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028995\n",
      "Epoch: 25/30. Train set: Average loss: 0.0279\n",
      "Epoch: 25/30. Validation set: Average loss: 0.0807\n",
      "Train: [0/1881 (0%)]\tLoss: 0.017085\n",
      "Train: [800/1881 (42%)]\tLoss: 0.029089\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028520\n",
      "Epoch: 26/30. Train set: Average loss: 0.0288\n",
      "Epoch: 26/30. Validation set: Average loss: 0.0807\n",
      "Train: [0/1881 (0%)]\tLoss: 0.036562\n",
      "Train: [800/1881 (42%)]\tLoss: 0.028792\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.028720\n",
      "Epoch: 27/30. Train set: Average loss: 0.0283\n",
      "Epoch: 27/30. Validation set: Average loss: 0.0806\n",
      "Train: [0/1881 (0%)]\tLoss: 0.046370\n",
      "Train: [800/1881 (42%)]\tLoss: 0.029515\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.029863\n",
      "Epoch: 28/30. Train set: Average loss: 0.0303\n",
      "Epoch: 28/30. Validation set: Average loss: 0.0806\n",
      "Train: [0/1881 (0%)]\tLoss: 0.029890\n",
      "Train: [800/1881 (42%)]\tLoss: 0.026534\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.029742\n",
      "Epoch: 29/30. Train set: Average loss: 0.0279\n",
      "Epoch: 29/30. Validation set: Average loss: 0.0808\n",
      "Train: [0/1881 (0%)]\tLoss: 0.006779\n",
      "Train: [800/1881 (42%)]\tLoss: 0.027961\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.027027\n",
      "Epoch: 30/30. Train set: Average loss: 0.0276\n",
      "Epoch: 30/30. Validation set: Average loss: 0.0809\n",
      "Train: [0/1881 (0%)]\tLoss: 0.086013\n",
      "Train: [800/1881 (42%)]\tLoss: 0.152255\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.116419\n",
      "Epoch: 1/30. Train set: Average loss: 0.1305\n",
      "Epoch: 1/30. Validation set: Average loss: 0.1095\n",
      "Train: [0/1881 (0%)]\tLoss: 0.151173\n",
      "Train: [800/1881 (42%)]\tLoss: 0.111557\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.111061\n",
      "Epoch: 2/30. Train set: Average loss: 0.1102\n",
      "Epoch: 2/30. Validation set: Average loss: 0.1087\n",
      "Train: [0/1881 (0%)]\tLoss: 0.107959\n",
      "Train: [800/1881 (42%)]\tLoss: 0.107766\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.107150\n",
      "Epoch: 3/30. Train set: Average loss: 0.1072\n",
      "Epoch: 3/30. Validation set: Average loss: 0.1197\n",
      "Train: [0/1881 (0%)]\tLoss: 0.122282\n",
      "Train: [800/1881 (42%)]\tLoss: 0.111597\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.096140\n",
      "Epoch: 4/30. Train set: Average loss: 0.1032\n",
      "Epoch: 4/30. Validation set: Average loss: 0.1050\n",
      "Train: [0/1881 (0%)]\tLoss: 0.115539\n",
      "Train: [800/1881 (42%)]\tLoss: 0.100085\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.092549\n",
      "Epoch: 5/30. Train set: Average loss: 0.0956\n",
      "Epoch: 5/30. Validation set: Average loss: 0.1055\n",
      "Train: [0/1881 (0%)]\tLoss: 0.100134\n",
      "Train: [800/1881 (42%)]\tLoss: 0.082513\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.084146\n",
      "Epoch: 6/30. Train set: Average loss: 0.0819\n",
      "Epoch: 6/30. Validation set: Average loss: 0.0963\n",
      "Train: [0/1881 (0%)]\tLoss: 0.060212\n",
      "Train: [800/1881 (42%)]\tLoss: 0.080229\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.078196\n",
      "Epoch: 7/30. Train set: Average loss: 0.0783\n",
      "Epoch: 7/30. Validation set: Average loss: 0.0917\n",
      "Train: [0/1881 (0%)]\tLoss: 0.077063\n",
      "Train: [800/1881 (42%)]\tLoss: 0.066790\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.065344\n",
      "Epoch: 8/30. Train set: Average loss: 0.0649\n",
      "Epoch: 8/30. Validation set: Average loss: 0.0856\n",
      "Train: [0/1881 (0%)]\tLoss: 0.112881\n",
      "Train: [800/1881 (42%)]\tLoss: 0.061162\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.056604\n",
      "Epoch: 9/30. Train set: Average loss: 0.0594\n",
      "Epoch: 9/30. Validation set: Average loss: 0.0826\n",
      "Train: [0/1881 (0%)]\tLoss: 0.076898\n",
      "Train: [800/1881 (42%)]\tLoss: 0.056250\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.056773\n",
      "Epoch: 10/30. Train set: Average loss: 0.0569\n",
      "Epoch: 10/30. Validation set: Average loss: 0.0818\n",
      "Train: [0/1881 (0%)]\tLoss: 0.088550\n",
      "Train: [800/1881 (42%)]\tLoss: 0.055453\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.052265\n",
      "Epoch: 11/30. Train set: Average loss: 0.0533\n",
      "Epoch: 11/30. Validation set: Average loss: 0.0803\n",
      "Train: [0/1881 (0%)]\tLoss: 0.073921\n",
      "Train: [800/1881 (42%)]\tLoss: 0.052817\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.048318\n",
      "Epoch: 12/30. Train set: Average loss: 0.0502\n",
      "Epoch: 12/30. Validation set: Average loss: 0.0807\n",
      "Train: [0/1881 (0%)]\tLoss: 0.046675\n",
      "Train: [800/1881 (42%)]\tLoss: 0.052149\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049461\n",
      "Epoch: 13/30. Train set: Average loss: 0.0508\n",
      "Epoch: 13/30. Validation set: Average loss: 0.0792\n",
      "Train: [0/1881 (0%)]\tLoss: 0.060523\n",
      "Train: [800/1881 (42%)]\tLoss: 0.045142\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.047204\n",
      "Epoch: 14/30. Train set: Average loss: 0.0465\n",
      "Epoch: 14/30. Validation set: Average loss: 0.0781\n",
      "Train: [0/1881 (0%)]\tLoss: 0.032288\n",
      "Train: [800/1881 (42%)]\tLoss: 0.047454\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.046115\n",
      "Epoch: 15/30. Train set: Average loss: 0.0467\n",
      "Epoch: 15/30. Validation set: Average loss: 0.0789\n",
      "Train: [0/1881 (0%)]\tLoss: 0.023627\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041810\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040705\n",
      "Epoch: 16/30. Train set: Average loss: 0.0415\n",
      "Epoch: 16/30. Validation set: Average loss: 0.0789\n",
      "Train: [0/1881 (0%)]\tLoss: 0.037626\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040008\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.044012\n",
      "Epoch: 17/30. Train set: Average loss: 0.0432\n",
      "Epoch: 17/30. Validation set: Average loss: 0.0784\n",
      "Train: [0/1881 (0%)]\tLoss: 0.041027\n",
      "Train: [800/1881 (42%)]\tLoss: 0.043053\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.042855\n",
      "Epoch: 18/30. Train set: Average loss: 0.0430\n",
      "Epoch: 18/30. Validation set: Average loss: 0.0789\n",
      "Train: [0/1881 (0%)]\tLoss: 0.030384\n",
      "Train: [800/1881 (42%)]\tLoss: 0.042796\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040429\n",
      "Epoch: 19/30. Train set: Average loss: 0.0417\n",
      "Epoch: 19/30. Validation set: Average loss: 0.0786\n",
      "Train: [0/1881 (0%)]\tLoss: 0.038484\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041566\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040612\n",
      "Epoch: 20/30. Train set: Average loss: 0.0404\n",
      "Epoch: 20/30. Validation set: Average loss: 0.0795\n",
      "Train: [0/1881 (0%)]\tLoss: 0.025778\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040464\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.043255\n",
      "Epoch: 21/30. Train set: Average loss: 0.0416\n",
      "Epoch: 21/30. Validation set: Average loss: 0.0786\n",
      "Train: [0/1881 (0%)]\tLoss: 0.028158\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039741\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041056\n",
      "Epoch: 22/30. Train set: Average loss: 0.0400\n",
      "Epoch: 22/30. Validation set: Average loss: 0.0794\n",
      "Train: [0/1881 (0%)]\tLoss: 0.036602\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039821\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.038960\n",
      "Epoch: 23/30. Train set: Average loss: 0.0402\n",
      "Epoch: 23/30. Validation set: Average loss: 0.0790\n",
      "Train: [0/1881 (0%)]\tLoss: 0.048876\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041109\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041171\n",
      "Epoch: 24/30. Train set: Average loss: 0.0413\n",
      "Epoch: 24/30. Validation set: Average loss: 0.0789\n",
      "Train: [0/1881 (0%)]\tLoss: 0.048308\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040831\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.042148\n",
      "Epoch: 25/30. Train set: Average loss: 0.0410\n",
      "Epoch: 25/30. Validation set: Average loss: 0.0787\n",
      "Train: [0/1881 (0%)]\tLoss: 0.092220\n",
      "Train: [800/1881 (42%)]\tLoss: 0.037717\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039712\n",
      "Epoch: 26/30. Train set: Average loss: 0.0390\n",
      "Epoch: 26/30. Validation set: Average loss: 0.0786\n",
      "Train: [0/1881 (0%)]\tLoss: 0.024260\n",
      "Train: [800/1881 (42%)]\tLoss: 0.036494\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041048\n",
      "Epoch: 27/30. Train set: Average loss: 0.0394\n",
      "Epoch: 27/30. Validation set: Average loss: 0.0787\n",
      "Train: [0/1881 (0%)]\tLoss: 0.019247\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041759\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037348\n",
      "Epoch: 28/30. Train set: Average loss: 0.0395\n",
      "Epoch: 28/30. Validation set: Average loss: 0.0787\n",
      "Train: [0/1881 (0%)]\tLoss: 0.055232\n",
      "Train: [800/1881 (42%)]\tLoss: 0.037639\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039635\n",
      "Epoch: 29/30. Train set: Average loss: 0.0391\n",
      "Epoch: 29/30. Validation set: Average loss: 0.0786\n",
      "Train: [0/1881 (0%)]\tLoss: 0.035802\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040579\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.035125\n",
      "Epoch: 30/30. Train set: Average loss: 0.0384\n",
      "Epoch: 30/30. Validation set: Average loss: 0.0786\n",
      "Train: [0/1881 (0%)]\tLoss: 0.142276\n",
      "Train: [800/1881 (42%)]\tLoss: 0.421070\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.125472\n",
      "Epoch: 1/30. Train set: Average loss: 0.2503\n",
      "Epoch: 1/30. Validation set: Average loss: 0.1196\n",
      "Train: [0/1881 (0%)]\tLoss: 0.136522\n",
      "Train: [800/1881 (42%)]\tLoss: 0.117013\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.114057\n",
      "Epoch: 2/30. Train set: Average loss: 0.1139\n",
      "Epoch: 2/30. Validation set: Average loss: 0.1120\n",
      "Train: [0/1881 (0%)]\tLoss: 0.071931\n",
      "Train: [800/1881 (42%)]\tLoss: 0.103149\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.098176\n",
      "Epoch: 3/30. Train set: Average loss: 0.1016\n",
      "Epoch: 3/30. Validation set: Average loss: 0.1134\n",
      "Train: [0/1881 (0%)]\tLoss: 0.143038\n",
      "Train: [800/1881 (42%)]\tLoss: 0.096300\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.094879\n",
      "Epoch: 4/30. Train set: Average loss: 0.0973\n",
      "Epoch: 4/30. Validation set: Average loss: 0.1043\n",
      "Train: [0/1881 (0%)]\tLoss: 0.059131\n",
      "Train: [800/1881 (42%)]\tLoss: 0.093826\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.087867\n",
      "Epoch: 5/30. Train set: Average loss: 0.0907\n",
      "Epoch: 5/30. Validation set: Average loss: 0.0987\n",
      "Train: [0/1881 (0%)]\tLoss: 0.124604\n",
      "Train: [800/1881 (42%)]\tLoss: 0.081848\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.080719\n",
      "Epoch: 6/30. Train set: Average loss: 0.0807\n",
      "Epoch: 6/30. Validation set: Average loss: 0.1930\n",
      "Train: [0/1881 (0%)]\tLoss: 0.117034\n",
      "Train: [800/1881 (42%)]\tLoss: 0.082187\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.077727\n",
      "Epoch: 7/30. Train set: Average loss: 0.0788\n",
      "Epoch: 7/30. Validation set: Average loss: 0.1003\n",
      "Train: [0/1881 (0%)]\tLoss: 0.083877\n",
      "Train: [800/1881 (42%)]\tLoss: 0.065030\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.065626\n",
      "Epoch: 8/30. Train set: Average loss: 0.0646\n",
      "Epoch: 8/30. Validation set: Average loss: 0.0953\n",
      "Train: [0/1881 (0%)]\tLoss: 0.064851\n",
      "Train: [800/1881 (42%)]\tLoss: 0.062778\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.063204\n",
      "Epoch: 9/30. Train set: Average loss: 0.0621\n",
      "Epoch: 9/30. Validation set: Average loss: 0.0953\n",
      "Train: [0/1881 (0%)]\tLoss: 0.059665\n",
      "Train: [800/1881 (42%)]\tLoss: 0.061034\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.061488\n",
      "Epoch: 10/30. Train set: Average loss: 0.0601\n",
      "Epoch: 10/30. Validation set: Average loss: 0.0959\n",
      "Train: [0/1881 (0%)]\tLoss: 0.069450\n",
      "Train: [800/1881 (42%)]\tLoss: 0.053946\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.054867\n",
      "Epoch: 11/30. Train set: Average loss: 0.0536\n",
      "Epoch: 11/30. Validation set: Average loss: 0.0946\n",
      "Train: [0/1881 (0%)]\tLoss: 0.057712\n",
      "Train: [800/1881 (42%)]\tLoss: 0.048597\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.048121\n",
      "Epoch: 12/30. Train set: Average loss: 0.0492\n",
      "Epoch: 12/30. Validation set: Average loss: 0.0916\n",
      "Train: [0/1881 (0%)]\tLoss: 0.091537\n",
      "Train: [800/1881 (42%)]\tLoss: 0.049313\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.045732\n",
      "Epoch: 13/30. Train set: Average loss: 0.0482\n",
      "Epoch: 13/30. Validation set: Average loss: 0.0907\n",
      "Train: [0/1881 (0%)]\tLoss: 0.040294\n",
      "Train: [800/1881 (42%)]\tLoss: 0.049583\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.048906\n",
      "Epoch: 14/30. Train set: Average loss: 0.0477\n",
      "Epoch: 14/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.024139\n",
      "Train: [800/1881 (42%)]\tLoss: 0.043897\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.045071\n",
      "Epoch: 15/30. Train set: Average loss: 0.0441\n",
      "Epoch: 15/30. Validation set: Average loss: 0.0920\n",
      "Train: [0/1881 (0%)]\tLoss: 0.042374\n",
      "Train: [800/1881 (42%)]\tLoss: 0.042105\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.043599\n",
      "Epoch: 16/30. Train set: Average loss: 0.0426\n",
      "Epoch: 16/30. Validation set: Average loss: 0.0908\n",
      "Train: [0/1881 (0%)]\tLoss: 0.008024\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041276\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040234\n",
      "Epoch: 17/30. Train set: Average loss: 0.0405\n",
      "Epoch: 17/30. Validation set: Average loss: 0.0912\n",
      "Train: [0/1881 (0%)]\tLoss: 0.038138\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039028\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040474\n",
      "Epoch: 18/30. Train set: Average loss: 0.0395\n",
      "Epoch: 18/30. Validation set: Average loss: 0.0916\n",
      "Train: [0/1881 (0%)]\tLoss: 0.035700\n",
      "Train: [800/1881 (42%)]\tLoss: 0.038482\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041309\n",
      "Epoch: 19/30. Train set: Average loss: 0.0399\n",
      "Epoch: 19/30. Validation set: Average loss: 0.0910\n",
      "Train: [0/1881 (0%)]\tLoss: 0.029219\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041516\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037832\n",
      "Epoch: 20/30. Train set: Average loss: 0.0391\n",
      "Epoch: 20/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.018529\n",
      "Train: [800/1881 (42%)]\tLoss: 0.037441\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037940\n",
      "Epoch: 21/30. Train set: Average loss: 0.0374\n",
      "Epoch: 21/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.033571\n",
      "Train: [800/1881 (42%)]\tLoss: 0.038211\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039271\n",
      "Epoch: 22/30. Train set: Average loss: 0.0380\n",
      "Epoch: 22/30. Validation set: Average loss: 0.0912\n",
      "Train: [0/1881 (0%)]\tLoss: 0.058065\n",
      "Train: [800/1881 (42%)]\tLoss: 0.037455\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037334\n",
      "Epoch: 23/30. Train set: Average loss: 0.0374\n",
      "Epoch: 23/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.026262\n",
      "Train: [800/1881 (42%)]\tLoss: 0.035211\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.042016\n",
      "Epoch: 24/30. Train set: Average loss: 0.0377\n",
      "Epoch: 24/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.035333\n",
      "Train: [800/1881 (42%)]\tLoss: 0.036981\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.035429\n",
      "Epoch: 25/30. Train set: Average loss: 0.0363\n",
      "Epoch: 25/30. Validation set: Average loss: 0.0914\n",
      "Train: [0/1881 (0%)]\tLoss: 0.058319\n",
      "Train: [800/1881 (42%)]\tLoss: 0.036293\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.035506\n",
      "Epoch: 26/30. Train set: Average loss: 0.0357\n",
      "Epoch: 26/30. Validation set: Average loss: 0.0912\n",
      "Train: [0/1881 (0%)]\tLoss: 0.022842\n",
      "Train: [800/1881 (42%)]\tLoss: 0.033394\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037379\n",
      "Epoch: 27/30. Train set: Average loss: 0.0361\n",
      "Epoch: 27/30. Validation set: Average loss: 0.0913\n",
      "Train: [0/1881 (0%)]\tLoss: 0.041103\n",
      "Train: [800/1881 (42%)]\tLoss: 0.033575\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.033912\n",
      "Epoch: 28/30. Train set: Average loss: 0.0341\n",
      "Epoch: 28/30. Validation set: Average loss: 0.0912\n",
      "Train: [0/1881 (0%)]\tLoss: 0.040317\n",
      "Train: [800/1881 (42%)]\tLoss: 0.037668\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.036778\n",
      "Epoch: 29/30. Train set: Average loss: 0.0375\n",
      "Epoch: 29/30. Validation set: Average loss: 0.0912\n",
      "Train: [0/1881 (0%)]\tLoss: 0.023599\n",
      "Train: [800/1881 (42%)]\tLoss: 0.038140\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037084\n",
      "Epoch: 30/30. Train set: Average loss: 0.0370\n",
      "Epoch: 30/30. Validation set: Average loss: 0.0911\n",
      "Train: [0/1881 (0%)]\tLoss: 0.526259\n",
      "Train: [800/1881 (42%)]\tLoss: 1.203823\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.147954\n",
      "Epoch: 1/30. Train set: Average loss: 0.5946\n",
      "Epoch: 1/30. Validation set: Average loss: 0.1310\n",
      "Train: [0/1881 (0%)]\tLoss: 0.118071\n",
      "Train: [800/1881 (42%)]\tLoss: 0.136953\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.118479\n",
      "Epoch: 2/30. Train set: Average loss: 0.1267\n",
      "Epoch: 2/30. Validation set: Average loss: 0.1133\n",
      "Train: [0/1881 (0%)]\tLoss: 0.122104\n",
      "Train: [800/1881 (42%)]\tLoss: 0.114669\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.105835\n",
      "Epoch: 3/30. Train set: Average loss: 0.1094\n",
      "Epoch: 3/30. Validation set: Average loss: 0.1089\n",
      "Train: [0/1881 (0%)]\tLoss: 0.108630\n",
      "Train: [800/1881 (42%)]\tLoss: 0.103341\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.103760\n",
      "Epoch: 4/30. Train set: Average loss: 0.1037\n",
      "Epoch: 4/30. Validation set: Average loss: 0.1114\n",
      "Train: [0/1881 (0%)]\tLoss: 0.081130\n",
      "Train: [800/1881 (42%)]\tLoss: 0.092671\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.098367\n",
      "Epoch: 5/30. Train set: Average loss: 0.0950\n",
      "Epoch: 5/30. Validation set: Average loss: 0.1024\n",
      "Train: [0/1881 (0%)]\tLoss: 0.111166\n",
      "Train: [800/1881 (42%)]\tLoss: 0.088895\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.083760\n",
      "Epoch: 6/30. Train set: Average loss: 0.0855\n",
      "Epoch: 6/30. Validation set: Average loss: 0.1219\n",
      "Train: [0/1881 (0%)]\tLoss: 0.091997\n",
      "Train: [800/1881 (42%)]\tLoss: 0.085757\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.074769\n",
      "Epoch: 7/30. Train set: Average loss: 0.0788\n",
      "Epoch: 7/30. Validation set: Average loss: 0.1098\n",
      "Train: [0/1881 (0%)]\tLoss: 0.073104\n",
      "Train: [800/1881 (42%)]\tLoss: 0.070942\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.070580\n",
      "Epoch: 8/30. Train set: Average loss: 0.0702\n",
      "Epoch: 8/30. Validation set: Average loss: 0.0961\n",
      "Train: [0/1881 (0%)]\tLoss: 0.084939\n",
      "Train: [800/1881 (42%)]\tLoss: 0.065545\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.065535\n",
      "Epoch: 9/30. Train set: Average loss: 0.0657\n",
      "Epoch: 9/30. Validation set: Average loss: 0.0964\n",
      "Train: [0/1881 (0%)]\tLoss: 0.039594\n",
      "Train: [800/1881 (42%)]\tLoss: 0.065014\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.059012\n",
      "Epoch: 10/30. Train set: Average loss: 0.0617\n",
      "Epoch: 10/30. Validation set: Average loss: 0.0952\n",
      "Train: [0/1881 (0%)]\tLoss: 0.043147\n",
      "Train: [800/1881 (42%)]\tLoss: 0.061497\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.063251\n",
      "Epoch: 11/30. Train set: Average loss: 0.0617\n",
      "Epoch: 11/30. Validation set: Average loss: 0.0951\n",
      "Train: [0/1881 (0%)]\tLoss: 0.048658\n",
      "Train: [800/1881 (42%)]\tLoss: 0.056211\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.057661\n",
      "Epoch: 12/30. Train set: Average loss: 0.0577\n",
      "Epoch: 12/30. Validation set: Average loss: 0.0943\n",
      "Train: [0/1881 (0%)]\tLoss: 0.048372\n",
      "Train: [800/1881 (42%)]\tLoss: 0.056836\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.054980\n",
      "Epoch: 13/30. Train set: Average loss: 0.0556\n",
      "Epoch: 13/30. Validation set: Average loss: 0.0937\n",
      "Train: [0/1881 (0%)]\tLoss: 0.051656\n",
      "Train: [800/1881 (42%)]\tLoss: 0.051877\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.049337\n",
      "Epoch: 14/30. Train set: Average loss: 0.0506\n",
      "Epoch: 14/30. Validation set: Average loss: 0.0953\n",
      "Train: [0/1881 (0%)]\tLoss: 0.033691\n",
      "Train: [800/1881 (42%)]\tLoss: 0.049423\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.045473\n",
      "Epoch: 15/30. Train set: Average loss: 0.0474\n",
      "Epoch: 15/30. Validation set: Average loss: 0.0957\n",
      "Train: [0/1881 (0%)]\tLoss: 0.035385\n",
      "Train: [800/1881 (42%)]\tLoss: 0.045704\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.045179\n",
      "Epoch: 16/30. Train set: Average loss: 0.0458\n",
      "Epoch: 16/30. Validation set: Average loss: 0.0951\n",
      "Train: [0/1881 (0%)]\tLoss: 0.056421\n",
      "Train: [800/1881 (42%)]\tLoss: 0.042359\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.043598\n",
      "Epoch: 17/30. Train set: Average loss: 0.0433\n",
      "Epoch: 17/30. Validation set: Average loss: 0.0951\n",
      "Train: [0/1881 (0%)]\tLoss: 0.033124\n",
      "Train: [800/1881 (42%)]\tLoss: 0.043376\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.046165\n",
      "Epoch: 18/30. Train set: Average loss: 0.0441\n",
      "Epoch: 18/30. Validation set: Average loss: 0.0949\n",
      "Train: [0/1881 (0%)]\tLoss: 0.027073\n",
      "Train: [800/1881 (42%)]\tLoss: 0.044867\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039891\n",
      "Epoch: 19/30. Train set: Average loss: 0.0428\n",
      "Epoch: 19/30. Validation set: Average loss: 0.0951\n",
      "Train: [0/1881 (0%)]\tLoss: 0.032836\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041381\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.044680\n",
      "Epoch: 20/30. Train set: Average loss: 0.0430\n",
      "Epoch: 20/30. Validation set: Average loss: 0.0954\n",
      "Train: [0/1881 (0%)]\tLoss: 0.045363\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040743\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.042695\n",
      "Epoch: 21/30. Train set: Average loss: 0.0417\n",
      "Epoch: 21/30. Validation set: Average loss: 0.0957\n",
      "Train: [0/1881 (0%)]\tLoss: 0.061045\n",
      "Train: [800/1881 (42%)]\tLoss: 0.044173\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039870\n",
      "Epoch: 22/30. Train set: Average loss: 0.0429\n",
      "Epoch: 22/30. Validation set: Average loss: 0.0957\n",
      "Train: [0/1881 (0%)]\tLoss: 0.045678\n",
      "Train: [800/1881 (42%)]\tLoss: 0.038790\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039926\n",
      "Epoch: 23/30. Train set: Average loss: 0.0393\n",
      "Epoch: 23/30. Validation set: Average loss: 0.0955\n",
      "Train: [0/1881 (0%)]\tLoss: 0.052786\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039174\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.039897\n",
      "Epoch: 24/30. Train set: Average loss: 0.0398\n",
      "Epoch: 24/30. Validation set: Average loss: 0.0955\n",
      "Train: [0/1881 (0%)]\tLoss: 0.074007\n",
      "Train: [800/1881 (42%)]\tLoss: 0.043107\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.038382\n",
      "Epoch: 25/30. Train set: Average loss: 0.0407\n",
      "Epoch: 25/30. Validation set: Average loss: 0.0955\n",
      "Train: [0/1881 (0%)]\tLoss: 0.046585\n",
      "Train: [800/1881 (42%)]\tLoss: 0.042187\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.040406\n",
      "Epoch: 26/30. Train set: Average loss: 0.0414\n",
      "Epoch: 26/30. Validation set: Average loss: 0.0953\n",
      "Train: [0/1881 (0%)]\tLoss: 0.030772\n",
      "Train: [800/1881 (42%)]\tLoss: 0.040357\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.038172\n",
      "Epoch: 27/30. Train set: Average loss: 0.0393\n",
      "Epoch: 27/30. Validation set: Average loss: 0.0954\n",
      "Train: [0/1881 (0%)]\tLoss: 0.017170\n",
      "Train: [800/1881 (42%)]\tLoss: 0.041857\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041048\n",
      "Epoch: 28/30. Train set: Average loss: 0.0413\n",
      "Epoch: 28/30. Validation set: Average loss: 0.0954\n",
      "Train: [0/1881 (0%)]\tLoss: 0.051818\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039485\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.041277\n",
      "Epoch: 29/30. Train set: Average loss: 0.0411\n",
      "Epoch: 29/30. Validation set: Average loss: 0.0955\n",
      "Train: [0/1881 (0%)]\tLoss: 0.074078\n",
      "Train: [800/1881 (42%)]\tLoss: 0.039817\n",
      "Train: [1600/1881 (85%)]\tLoss: 0.037470\n",
      "Epoch: 30/30. Train set: Average loss: 0.0400\n",
      "Epoch: 30/30. Validation set: Average loss: 0.0954\n"
     ]
    }
   ],
   "source": [
    "dimensionalities = [2, 256, 1024, 8192, 65536]\n",
    "maps = []\n",
    "prec1_alls = []\n",
    "prec5_alls = []\n",
    "\n",
    "for dimensionality in dimensionalities:\n",
    "    def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n",
    "            start_epoch=0, save_gif=False):\n",
    "        \"\"\"\n",
    "        Loaders, model, loss function and metrics should work together for a given task,\n",
    "        i.e. The model should be able to process data output of loaders,\n",
    "        loss function should process target output of loaders and outputs from the model\n",
    "        Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
    "        Siamese network: Siamese loader, siamese model, contrastive loss\n",
    "        Online triplet learning: batch loader, embedding model, online triplet loss\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(0, start_epoch):\n",
    "            scheduler.step()\n",
    "\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            scheduler.step()\n",
    "\n",
    "            # Train stage\n",
    "            train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n",
    "            message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "            \n",
    "            # Test stage\n",
    "            val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
    "                                                                                    val_loss)\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message)\n",
    "            \n",
    "        if save_gif:\n",
    "            train_embeddings_cl, train_labels_cl = extract_embeddings(original_train_loader, model)\n",
    "            val_embeddings_cl, val_labels_cl = extract_embeddings(original_test_loader, model)\n",
    "            \n",
    "            for dim_reduction in ['umap']:\n",
    "                plot_embeddings(train_embeddings_cl, train_labels_cl, title='Train embeddings', save_name=f'train_embedding_{dimensionality}_{dim_reduction}_{str(epoch).zfill(3)}.png')\n",
    "                plot_embeddings(val_embeddings_cl, val_labels_cl, title='Test embeddings', save_name=f'test_embedding_{dimensionality}_{dim_reduction}_{str(epoch).zfill(3)}.png')\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "\n",
    "\n",
    "    def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        losses = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            target = target if len(target) > 0 else None\n",
    "            if not type(data) in (tuple, list):\n",
    "                data = (data,)\n",
    "            if cuda:\n",
    "                data = tuple(d.cuda() for d in data)\n",
    "                if target is not None:\n",
    "                    target = target.cuda()\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(*data)\n",
    "\n",
    "            if type(outputs) not in (tuple, list):\n",
    "                outputs = (outputs,)\n",
    "\n",
    "            loss_inputs = outputs\n",
    "            if target is not None:\n",
    "                target = (target,)\n",
    "                loss_inputs += target\n",
    "\n",
    "            loss_outputs = loss_fn(*loss_inputs)\n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            losses.append(loss.item())\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric(outputs, target, loss_outputs)\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "                for metric in metrics:\n",
    "                    message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "                print(message)\n",
    "                losses = []\n",
    "\n",
    "        total_loss /= (batch_idx + 1)\n",
    "        return total_loss, metrics\n",
    "\n",
    "\n",
    "    def test_epoch(val_loader, model, loss_fn, cuda, metrics):\n",
    "        with torch.no_grad():\n",
    "            for metric in metrics:\n",
    "                metric.reset()\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                target = target if len(target) > 0 else None\n",
    "                if not type(data) in (tuple, list):\n",
    "                    data = (data,)\n",
    "                if cuda:\n",
    "                    data = tuple(d.cuda() for d in data)\n",
    "                    if target is not None:\n",
    "                        target = target.cuda()\n",
    "\n",
    "                outputs = model(*data)\n",
    "\n",
    "                if type(outputs) not in (tuple, list):\n",
    "                    outputs = (outputs,)\n",
    "                loss_inputs = outputs\n",
    "                if target is not None:\n",
    "                    target = (target,)\n",
    "                    loss_inputs += target\n",
    "\n",
    "                loss_outputs = loss_fn(*loss_inputs)\n",
    "                loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                for metric in metrics:\n",
    "                    metric(outputs, target, loss_outputs)\n",
    "\n",
    "        return val_loss, metrics\n",
    "\n",
    "\n",
    "    class EmbeddingNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(EmbeddingNet, self).__init__()\n",
    "            # Change for one channel instead of 3 if input is grayscale\n",
    "            self.convnet = nn.Sequential(nn.Conv2d(3, 32, 5), nn.PReLU(),\n",
    "                                        nn.MaxPool2d(2, stride=2),\n",
    "                                        nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                        nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "            self.fc = nn.Sequential(nn.Linear(238144, 256),\n",
    "                                    nn.PReLU(),\n",
    "                                    nn.Linear(256, 256),\n",
    "                                    nn.PReLU(),\n",
    "                                    nn.Linear(256, dimensionality)\n",
    "                                    )\n",
    "\n",
    "        def forward(self, x):\n",
    "            output = self.convnet(x)\n",
    "            output = output.view(output.size()[0], -1)\n",
    "            output = self.fc(output)\n",
    "            return output\n",
    "\n",
    "        def get_embedding(self, x):\n",
    "            return self.forward(x)\n",
    "        \n",
    "    class SiameseNet(nn.Module):\n",
    "        def __init__(self, embedding_net):\n",
    "            super(SiameseNet, self).__init__()\n",
    "            self.embedding_net = embedding_net\n",
    "            \n",
    "        def forward(self, x1, x2):\n",
    "            output1 = self.embedding_net(x1)\n",
    "            output2 = self.embedding_net(x2)\n",
    "            \n",
    "            return output1, output2\n",
    "\n",
    "        def get_embedding(self, x):\n",
    "            return self.embedding_net(x)\n",
    "\n",
    "    def extract_embeddings(dataloader, model):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            embeddings = np.zeros((len(dataloader.dataset), dimensionality))\n",
    "            labels = np.zeros(len(dataloader.dataset))\n",
    "            k = 0\n",
    "            for images, target in dataloader:\n",
    "                if cuda:\n",
    "                    images = images.cuda()\n",
    "                embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "                labels[k:k+len(images)] = target.numpy()\n",
    "                k += len(images)\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "    def plot_embeddings(embeddings, targets, title='', save_name='', xlim=None, ylim=None, dim_reduction='pca'):\n",
    "        if embeddings.shape[1] > 2:\n",
    "            if dim_reduction == 'pca':\n",
    "                pca = PCA(n_components=2)\n",
    "                embeddings = pca.fit_transform(embeddings)\n",
    "            elif dim_reduction == 'tsne':\n",
    "                embeddings = TSNE(n_components=2).fit_transform(embeddings)\n",
    "            elif dim_reduction == 'umap':\n",
    "                embeddings = umap.UMAP().fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(title)\n",
    "        for i in range(8):\n",
    "            inds = np.where(targets==i)[0]\n",
    "            plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "        if xlim:\n",
    "            plt.xlim(xlim[0], xlim[1])\n",
    "        if ylim:\n",
    "            plt.ylim(ylim[0], ylim[1])\n",
    "        plt.legend(mnist_classes, loc=\"upper right\")\n",
    "        if save_name != '':\n",
    "            plt.savefig(f'./outputs_task_b/{save_name}')\n",
    "            plt.close()\n",
    "\n",
    "    margin = 1.\n",
    "    embedding_net = EmbeddingNet()\n",
    "    model = SiameseNet(embedding_net)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    loss_fn = ContrastiveLoss(margin)\n",
    "    lr = 1e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "    n_epochs = 30\n",
    "    log_interval = 100\n",
    "\n",
    "    train_losses, val_losses = fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, save_gif=True)\n",
    "\n",
    "    # storing the image representations\n",
    "    im_indices = []\n",
    "    im_representations = []\n",
    "    embeddings_indexed = []\n",
    "    labels_indexed = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(original_train_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            embeddings = model.get_embedding(data).cpu().numpy()\n",
    "            \n",
    "            for idx, (im, label) in enumerate(zip(data, labels)):\n",
    "                im = im.permute(0, 1, 2).cpu().numpy()\n",
    "                label = label.cpu().numpy()\n",
    "                embedding = embeddings[idx]\n",
    "                im_indices.append(mnist_classes[label])\n",
    "                im_representations.append(im)\n",
    "                embeddings_indexed.append(embedding)\n",
    "                labels_indexed.append(label)\n",
    "                \n",
    "    embeddings_indexed = np.array(embeddings_indexed)\n",
    "    labels_indexed = np.array(labels_indexed)\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5).fit(embeddings_indexed, labels_indexed)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "\n",
    "    test_embeddings, test_labels = extract_embeddings(original_test_loader, model)\n",
    "    train_embeddings, train_labels = extract_embeddings(original_train_loader, model) \n",
    "\n",
    "\n",
    "    k = 10 # number of neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_embeddings, train_labels)\n",
    "    distances, indices = knn.kneighbors(test_embeddings)\n",
    "\n",
    "    labels_train = train_labels\n",
    "    labels_val = test_labels\n",
    "\n",
    "    val_targets = np.uint8(labels_val)\n",
    "    train_targets = np.uint8(labels_train)\n",
    "\n",
    "    # Compute the evaluation metrics\n",
    "    APs = []\n",
    "    Precisions_at_1 = []\n",
    "    Precisions_at_5 = []\n",
    "\n",
    "    # Compute MAP\n",
    "    # Convert integer targets to binary targets\n",
    "    binary_val_targets = np.zeros((test_embeddings.shape[0], 8))\n",
    "    binary_val_targets[np.arange(test_embeddings.shape[0]), val_targets] = 1\n",
    "\n",
    "    binary_train_targets = np.zeros((test_embeddings.shape[0], 8))\n",
    "    binary_train_targets[np.arange(test_embeddings.shape[0])[:,None], train_targets[indices]] = 1\n",
    "\n",
    "    # Compute average precision \n",
    "    for i in range(test_embeddings.shape[0]):\n",
    "        AP = average_precision_score(binary_val_targets[i], binary_train_targets[i])\n",
    "        APs.append(AP)\n",
    "\n",
    "    MAP = np.mean(APs)\n",
    "\n",
    "    for i, (dists, idxs, target) in enumerate(zip(distances, indices, val_targets)):\n",
    "        # Compute the precision at 1\n",
    "        if train_targets[idxs[0]] == target:\n",
    "            Precisions_at_1.append(1.0)\n",
    "        else:\n",
    "            Precisions_at_1.append(0.0)\n",
    "\n",
    "        # Compute the precision at 5\n",
    "        Precisions_at_5 = []\n",
    "        for idx, target in zip(indices, val_targets):\n",
    "            hits = np.isin(train_targets[idx][:5], target)\n",
    "            precision_at_5 = np.sum(hits) / 5.0\n",
    "            Precisions_at_5.append(precision_at_5)\n",
    "\n",
    "    # Compute the precision at 1 and precision at 5\n",
    "    Prec_1 = np.mean(Precisions_at_1)\n",
    "    Prec_5 = np.mean(Precisions_at_5)\n",
    "\n",
    "    maps.append(MAP)\n",
    "    prec1_alls.append(Prec_1)\n",
    "    prec5_alls.append(Prec_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46980586534489877, 0.6735336637752994, 0.6444547707558861, 0.6185357290375877, 0.5908199091284593]\n",
      "[0.5105328376703842, 0.6914498141263941, 0.7125154894671624, 0.6728624535315985, 0.6406443618339529]\n",
      "[0.5102850061957869, 0.6855018587360595, 0.7028500619578686, 0.6619578686493185, 0.626270136307311]\n"
     ]
    }
   ],
   "source": [
    "print(maps)\n",
    "print(prec1_alls)\n",
    "print(prec5_alls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x = range(len(maps))\n",
    "width = 0.2\n",
    "ax.bar(x, maps, width, label='mAP')\n",
    "ax.bar([i+width for i in x], prec1_alls, width, label='Precision@1')\n",
    "ax.bar([i+width*2 for i in x], prec5_alls, width, label='Precision@5')\n",
    "\n",
    "ax.set_xticks([i + width for i in x])\n",
    "ax.set_xticklabels([2, 256, 1024, 8192, 65536])\n",
    "\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Dimensionalities')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Image retrieval performance using different embedding dimensions')\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_fn = [0, 0.5, 1, 2, 5, 20, 50]\n",
    "maps = [0.16719183336283705, 0.445518380834366, 0.42778810408921936, 0.49951466336224704, 0.41368236266005787, 0.375784799669558, 0.3417625538443383]\n",
    "prec1_alls = [0.13135068153655513, 0.49566294919454773, 0.46220570012391576, 0.5390334572490706, 0.45848822800495664, 0.4485749690210657, 0.4200743494423792]\n",
    "prec5_alls = [0.14671623296158612, 0.49516728624535317, 0.46617100371747217, 0.5531598513011152, 0.4703841387856258, 0.45204460966542753, 0.4096654275092937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x = range(len(maps))\n",
    "width = 0.2\n",
    "ax.bar(x, maps, width, label='mAP')\n",
    "ax.bar([i+width for i in x], prec1_alls, width, label='Precision@1')\n",
    "ax.bar([i+width*2 for i in x], prec5_alls, width, label='Precision@5')\n",
    "\n",
    "ax.set_xticks([i + width for i in x])\n",
    "ax.set_xticklabels(losses_fn)\n",
    "\n",
    "# Axis labels and title\n",
    "ax.set_xlabel('Margins')\n",
    "ax.set_ylabel('Performance')\n",
    "ax.set_title('Image retrieval performance using different margins for the Contrastive Loss function')\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
