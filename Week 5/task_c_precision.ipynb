{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task c) Use BERT as Feature Extractor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models, ops\n",
    "from typing import Any, Callable, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50().cuda()\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_features, 512))\n",
    "\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load('./weights-resnet.pth'))\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define a simple linear layer to map BERT hidden state to caption embedding\n",
    "class CaptionEncoder(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(CaptionEncoder, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.linear = nn.Sequential(nn.Linear(768, 512))\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_embedding = last_hidden_state[:, 0, :]\n",
    "        caption_embedding = self.linear(cls_embedding)\n",
    "        return caption_embedding\n",
    "    \n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "import re\n",
    "\n",
    "epochs = 3\n",
    "device = 'cuda'\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create caption encoder and optimizer\n",
    "caption_encoder = CaptionEncoder(bert_model).cuda()\n",
    "caption_encoder = caption_encoder.to(device)\n",
    "#caption_encoder.load_state_dict(torch.load('./weights-caption-encoder-definitive.pth'))\n",
    "# Our base model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-gpu\n",
    "import faiss            \n",
    "PATH_TRAIN = \"./COCO/train2014/\"\n",
    "PATH_VALID = \"./COCO/val2014/\"\n",
    "ANNOTATIONS = \"./COCO/mcv_image_retrieval_annotations.json\"\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "with open(ANNOTATIONS, 'r') as j:\n",
    "            contents = json.loads(j.read())\n",
    "\n",
    "coco = COCO('./COCO/instances_train2014.json')\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    " # Buiid Database\n",
    "database_id = []\n",
    "database = []\n",
    "image_ids_database = set()\n",
    "\n",
    "\n",
    "for category in contents['database']:\n",
    "        image_ids_database.update(contents['database'][category])\n",
    "\n",
    "image_ids_database = sorted(list(image_ids_database))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_id in image_ids_database:\n",
    "           filename = coco.loadImgs(image_id)[0][\"file_name\"]\n",
    "           im = Image.open(os.path.join(PATH_TRAIN,filename)).convert('RGB')\n",
    "           im = im.resize((224,224))\n",
    "           im = torch.tensor(np.array([val_transforms(im).numpy()])).cuda()\n",
    "           output = model(im)\n",
    "           output = output.detach().cpu().numpy().reshape(-1, np.prod(output.size()[1:]))\n",
    "           database.append(output)\n",
    "           database_id.append(image_id)\n",
    "           \n",
    "database = np.asarray(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding database to index\n",
      "Database added to index\n"
     ]
    }
   ],
   "source": [
    "database = database.reshape((database.shape[0], database.shape[1]*database.shape[2]))\n",
    "index = faiss.IndexFlatL2(database.shape[1])\n",
    "print(\"Adding database to index\")\n",
    "index.add(database)\n",
    "print(\"Database added to index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Buliding list of queries...\")\n",
    "#Build list of queries\n",
    "queries = []\n",
    "queries_id = []\n",
    "queries_id_test = set()\n",
    "PATH_VALID = \"./COCO/val2014/\"\n",
    "\n",
    "for category in contents[\"database\"]:\n",
    "       queries_id_test.update(contents[\"database\"][category])\n",
    "\n",
    "queries_caption = []\n",
    "queries_id_test = sorted(list(queries_id_test))\n",
    "\n",
    "coco_captions = COCO('./COCO/captions_train2014.json')\n",
    "caption_encoder.eval()\n",
    "with torch.no_grad():\n",
    "       for idx, image_id in enumerate(queries_id_test):\n",
    "              captions = [ann[\"caption\"] for ann in coco_captions.loadAnns(coco_captions.getAnnIds(image_id))]\n",
    "              print(idx, \"/\", len(queries_id_test))\n",
    "              for caption in captions:\n",
    "                     queries_id.append(image_id)\n",
    "                     caption = tokenizer(caption.lower(), return_tensors=\"pt\", padding = True)\n",
    "                     input_ids = caption[\"input_ids\"].to(device)\n",
    "                     attention_mask = caption[\"attention_mask\"].to(device)\n",
    "                     embedding = caption_encoder(input_ids, attention_mask)\n",
    "                     embedding = embedding.detach().cpu().numpy().reshape(-1,np.prod(embedding.size()[1:]))\n",
    "                     queries.append(embedding)\n",
    "        \n",
    "queries = np.asarray(queries)\n",
    "\n",
    "queries = queries.reshape((queries.shape[0], queries.shape[1]*queries.shape[2]))\n",
    "\n",
    "print(\"Searching K neighbors for each query\")\n",
    "D, I = index.search(queries, 5)\n",
    "print(\"Finished searching\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = []    \n",
    "for i in range(I.shape[0]): # para cada query\n",
    "    TP = 0.0\n",
    "    retrieved = 0.0\n",
    "    y_true = []\n",
    "    for j in range(I[i].shape[0]): #para cada una de las 5 imágenes que ha devuelto FAISS / KNN\n",
    "        if queries_id[i] == database_id[I[i][j]]: #si el id de la imagen se corresponde la id de la query\n",
    "            TP += 1\n",
    "            y_true.append(1)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "        retrieved += 1\n",
    "    precision.append(TP/retrieved)\n",
    "\n",
    "precision = np.asarray(precision)\n",
    "precision = np.mean(precision)\n",
    "print(precision)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics \n",
    "average_precisions = []    \n",
    "for i in range(I.shape[0]): # para cada query\n",
    "    y_true = []\n",
    "    y_scores = [1.0, 0.8 ,0.6, 0.4, 0.2]\n",
    "    for j in range(I[i].shape[0]): #para cada una de las 5 imágenes que ha devuelto FAISS / KNN\n",
    "        if queries_id[i] == database_id[I[i][j]]: #si el id de la imagen se corresponde la id de la query\n",
    "            y_true.append(1)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "    average_precisions.append(sklearn.metrics.average_precision_score(y_true, y_scores))\n",
    "\n",
    "average_precision = np.asarray(average_precisions)\n",
    "print(np.mean(average_precision))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
